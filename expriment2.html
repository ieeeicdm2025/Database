<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta name="description" content="Academic Webpage" />
    <meta name="author" content="senli1073" />
    <title id="title"></title>

    <!-- Icon -->
    <link rel="icon" type="image/x-icon" href="static/assets/favicon.ico" />

    <!-- Bootstrap icons-->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.5.0/font/bootstrap-icons.css" rel="stylesheet" />

    <!-- Google fonts-->
    <link rel="preconnect" href="https://fonts.gstatic.com" />
    <link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,wght@0,600;1,600&amp;display=swap"
        rel="stylesheet" />
    <link
        href="https://fonts.googleapis.com/css2?family=Mulish:ital,wght@0,300;0,500;0,600;0,700;1,300;1,500;1,600;1,700&amp;display=swap"
        rel="stylesheet" />
    <link href="https://fonts.googleapis.com/css2?family=Kanit:ital,wght@0,400;1,400&amp;display=swap"
        rel="stylesheet" />

    <!-- Core theme CSS (includes Bootstrap)-->
    <link type="text/css" href="static/css/styles.css" rel="stylesheet" />
    <link type="text/css" href="static/css/main.css" rel="stylesheet" />

    <!-- Bootstrap core JS-->
    <script type="text/javascript" src="static/js/bootstrap.bundle.min.js"></script>

    <style>
        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ddd;
            font-family: monospace;
        }

        code {
            color: #333;
        }
    </style>

    <!-- For Compatability -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Markdown -->
    <script type="text/javascript" src="static/js/marked.min.js"></script>

    <!-- Mathematics -->
    <script>
        // See https://docs.mathjax.org/en/latest/index.html for more details.
        MathJax = {
            tex: {
                packages: {},              // extensions to use
                inlineMath: [              // start/end delimiter pairs for in-line math
                    ['$', '$'],
                    ['\\(', '\\)']
                ],
                displayMath: [             // start/end delimiter pairs for display math
                    ['$$', '$$'],
                    ['\\[', '\\]']
                ],
                processEscapes: false,      // use \$ to produce a literal dollar sign
                processEnvironments: true, // process \begin{xxx}...\end{xxx} outside math mode
                processRefs: true,         // process \ref{...} outside of math mode
                digits: /^(?:[0-9]+(?:\{,\}[0-9]{3})*(?:\.[0-9]*)?|\.[0-9]+)/,    // pattern for recognizing numbers
                tags: 'all',              // or 'ams' or 'all'
                tagSide: 'right',          // side for \tag macros
                tagIndent: '0.8em',        // amount to indent tags
                useLabelIds: true,         // use label name rather than tag for ids
                maxMacros: 10000,          // maximum number of macro substitutions per expression
                maxBuffer: 5 * 1024,       // maximum size for the internal TeX string (5K)
                // baseURL:                   // URL for use with links to tags (when there is a <base> tag in effect)
                // (document.getElementsByTagName('base').length === 0) ? '' : String(document.location).replace(/#.*$/, ''),
                formatError:               // function called when TeX syntax errors occur
                    (jax, err) => jax.formatError(err)
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script"
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <!-- Core JS-->
    <script type="text/javascript" src="static/js/scripts.js"></script>
    <script type="text/javascript" src="static/js/js-yaml.min.js"></script>

</head>

<body id="page-top">
    <!-- Navigation-->
    <nav class="header navbar navbar-expand-lg navbar-light fixed-top shadow-sm" id="mainNav">
        <div class="container px-5">
            <a id="page-top-title" class="navbar-brand fw-bold" href="#page-top"></a>
            <!-- <a href="#page-top"><img src="static/assets/img/CUMT_LOGO.svg" style="width: 11rem;"></a> -->
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                MENU
                <i class="bi-list"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ms-auto me-4 my-3 my-lg-0">
                    <li class="nav-item">
                        <a class="nav-link me-lg-3" href="index.html" style="font-size: 30px;">首页</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link me-lg-3" href="expriment.html" style="font-size: 30px;">实验指南</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link me-lg-3" href="ppt.html" style="font-size: 30px;"><strong>PPT</strong></a>
                    </li>

                </ul>
            </div>
        </div>
    </nav>

    <!-- Top Section -->
    <section class="top-section" style="background-image: url('static/assets/img/background.jpeg');">
        <div class="top-section-content">
            <div class="container px-5">
                <h2 id="top-section-bg-text" class="text-white display-3 lh-1 font-alt"></h2>
            </div>
        </div>
    </section>
    <!-- Top Section -->

    <!-- Photo -->
    <div class="container px-5">
        <div id="avatar">
            <img class="shadow" src="static/assets/img/photo.png">
        </div>
    </div>
    <!-- Photo -->

    <!-- Home -->
    <section class="bg-gradient-primary-to-secondary-light mt5 md5" id="home">
        <div class="container px-5">
            <header>
                <h2 id="home-subtitle"> </h2>
                <!-- <span class="bi bi-list"></span> -->
            </header>
            <div class="main-body" id="home-md"></div>
        </div>
    </section>
    <!-- Home -->



    <!-- Awards -->
    <section class="bg-gradient-primary-to-secondary-light mt5 md5" id="awards">
        <div class="container px-5">
            <header>
                <h2 id="awards-subtitle" style="font-size: 48px;"><i class="bi bi-award-fill"></i> 实验指南(二)：Hadoop和Spark
                </h2>
            </header>

            <div style="height: 64px;"></div>

            <div class="main-body" id="awards-md">


                <h1 id="hadoop"><strong>一、Hadoop</strong></h1>
                <div style="height: 32px;"></div>
                <h2 id="1-2"><strong>1、简介</strong></h2>
                <p>Hadoop 是一个开源的分布式计算框架，用于处理大量的数据集。它是由 Apache 软件基金会维护，旨在通过简单的编程模型将大量数据分布到不同的计算机节点上，从而实现数据的分布式存储和计算。</p>
                <img src="static/assets/img/hadoop_spark/hadoop_achi1.png" alt="hadoop_achi1" style="width: 100%; height: auto;"/>
                <br><br>
                <h2 id="2hadoop"><strong>2、Hadoop的核心概念</strong></h2>
                <p><strong>（1）HDFS（Hadoop Distributed File System）</strong></p>
                <ul>
                    <li>HDFS 是 Hadoop 的分布式存储系统，用于存储大数据。</li>
                    <li>块：HDFS将文件切分成多个块（block），并将这些块分布到集群中的不同节点上。每个块的默认大小通常是 128MB 或 256MB，可以通过配置文件调整</li>
                    <li>NameNode（主节点）：是 HDFS 的核心组件之一，负责管理文件系统的元数据（metadata）。元数据包括文件和块之间的映射关系、每个块的副本位置、目录结构等信息。</li>
                    <li>DataNode（数据节点）：DataNode 是 HDFS 中的数据存储节点，负责存储实际的文件数据块。每个 DataNode 存储文件的多个块，并周期性地向 NameNode
                        报告存储的块的健康状态。</li>
                    <li>副本（Replication）：为了保证数据的高可用性，HDFS 会在集群中的多个节点上存储同一个数据块的副本。默认情况下，每个块会存储 3 个副本（可以通过配置修改）。</li>
                </ul>
                <p><strong>（2）MapReduce</strong></p>
                <ul>
                    <li>MapReduce 是 Hadoop 的核心计算模型，用于处理大规模数据集。它分为两个阶段：</li>
                    <li>Map 阶段：输入数据被拆分成多个片段，并并行地处理每个数据片段，生成中间结果。</li>
                    <li>Reduce 阶段：将来自 Map 阶段的结果进行合并，得到最终输出</li>
                </ul>
                <p><strong>（3）YARN（Yet Another Resource Negotiator）</strong></p>
                <ul>
                    <li>YARN 是 Hadoop 的资源管理层，负责管理集群中的资源（CPU、内存等）并调度任务执行。它提供了一个统一的资源管理框架，支持 MapReduce、Spark
                        等多种计算框架在同一集群上运行。</li>
                    <li>资源管理器（ResourceManager, RM）：负责全局的资源管理和任务调度</li>
                    <li>节点管理器（NodeManager, NM）：负责管理每个集群节点上的资源和任务执行。</li>
                    <li>应用程序管理器（ApplicationMaster, AM）：ApplicationMaster 是每个应用程序（作业）对应的一个实例，负责管理应用程序的生命周期</li>
                    <li>容器（Container）：是 YARN 中资源分配的基本单位，它封装了一个作业的资源需求（如 CPU、内存等）以及运行作业的环境（如环境变量、JAR 文件等）</li>
                </ul>
                <img src="static/assets/img/hadoop_spark/hadoop_achi2.png" alt="hadoop_achi2" style="width: 100%; height: auto;"/>
                <br><br>
                <h2 id="3minikubehadoop"><strong>3、使用minikube部署Hadoop</strong></h2>
                <p><strong>（1）向minikube中导入所需的镜像</strong></p>
                <pre><code>minikube image load spark-hadoop-numpy:latest</code></pre>
                <ul>
                    <li>进入minikube查看是否导入</li>
                </ul>
                <pre><code>minikube ssh
docker images</code></pre>
                <img src="static/assets/img/hadoop_spark/hadoop_image.png" alt="hadoop_image" style="width: 100%; height: auto;"/>
                <br><br>
                <p><strong>（2）使用hadoop-cluster.yaml文件部署Hadoop，并查看pod状态验证其是否正常运行</strong></p>
                <pre><code>kubectl apply -f hadoop-cluster.yaml
kubectl get pods</code></pre>
                <img src="static/assets/img/hadoop_spark/hadoop_run.png" alt="hadoop_run" style="width: 100%; height: auto;"/>
                <br><br>
                <p><strong>（3）设置端口转发以访问Hadoop的webui</strong></p>
                <ul>
                    <li>设置转发以查看resourcemanager的webui：</li>
                </ul>
                <pre><code>kubectl port-forward svc/resourcemanager 8088:8088
</code></pre>
                <ul>
                    <li>使用kubectl的本地端口转发功能来查看namenode的webui。这个转发命令行界面需要一直保持转发状态，停止转发后就无法访问webui了：</li>
                </ul>
                <pre><code>kubectl port-forward svc/namenode 9870:9870
</code></pre>
                <ul>
                    <li>通过下面的url在浏览器访问：</li>
                </ul>
                <p><a href="http://localhost:8088/" target="_blank">http://localhost:8088/</a></p>
                <p><a href="http://localhost:9870/" target="_blank">http://localhost:9870/</a></p>
                <img src="static/assets/img/hadoop_spark/hadoop_webui1.png" alt="hadoop_webui1" style="width: 100%; height: auto;"/>
                <br><br>
                <img src="static/assets/img/hadoop_spark/hadoop_webui2.png" alt="hadoop_webui2" style="width: 100%; height: auto;"/>
                <br><br>
                <p><strong>（4）运行Hadoop的示例程序wordcount</strong></p>
                <ul>
                    <li>wordcount 示例程序是一个经典的示例，通常用于展示如何使用 Hadoop 进行分布式计算，它通过 MapReduce 模型统计输入文件中每个单词出现的次数。</li>
                    <li>Mapper：负责读取输入数据，将数据转换为键值对。</li>
                    <li>Reducer：负责将相同键的值进行汇总，,统计单词的出现次数。</li>
                    <li>进入namenode的命令行bash，下面的namenode-78587cfc8f-clmkd需要替换为自己的pod名称，可以通过kubectl get pods来查看name的pod名称
                    </li>
                </ul>
                <pre><code>kubectl exec -it namenode-78587cfc8f-clmkd -- bash
</code></pre>
                <ul>
                    <li>创建input.txt文件，作为wordcount程序的输入</li>
                </ul>
                <pre><code>echo -e "Hello Hadoop\nHello Kubernetes" &gt; input.txt
</code></pre>
                <ul>
                    <li>在hdfs上创建目录/input</li>
                </ul>
                <pre><code>hdfs dfs -mkdir -p /input
</code></pre>
                <ul>
                    <li>上传input.txt文件到HDFS</li>
                </ul>
                <pre><code>hdfs dfs -put input.txt /input/
</code></pre>
                <ul>
                    <li>运行wordcount程序</li>
                </ul>
                <pre><code>hadoop jar /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar wordcount /input/input.txt /output
</code></pre>
<img src="static/assets/img/hadoop_spark/hadoop_wc.png" alt="hadoop_wc" style="width: 100%; height: auto;"/>
<br><br>
                <p><strong>（5）运行k-means算法的mapreduce版本</strong></p>
                <ul>
                    <li>将所需的文件与jar包传入namenode，下面命令中的namenode-78587cfc8f-sl48f请替换为自己的namenode所在的pod名称</li>
                    <li>传入需要处理的数据文件data.txt、应用程序的jar包k-means-1.0-SNAPSHOT.jar、环境配置文件config.xml</li>
                </ul>
                <pre><code>kubectl cp data.txt namenode-78587cfc8f-sl48f:/opt/hadoop/
kubectl cp k-means-1.0-SNAPSHOT.jar namenode-78587cfc8f-sl48f:/opt/hadoop/
kubectl cp config.xml namenode-78587cfc8f-sl48f:/opt/hadoop/
</code></pre>
                <ul>
                    <li>进入namenode的命令行bash，同样的，下面命令中的namenode-78587cfc8f-sl48f请替换为自己的namenode所在的pod名称</li>
                </ul>
                <pre><code>kubectl exec -it namenode-78587cfc8f-sl48f -- /bin/bash
</code></pre>
                <ul>
                    <li>进入传入文件所在的文件夹（<strong>这一步不要忘记，不然找不到文件</strong>）</li>
                </ul>
                <pre><code>cd /opt/hadoop/
</code></pre>
                <ul>
                    <li>上传所需文件到HDFS</li>
                </ul>
                <pre><code>hadoop fs -put k-means-1.0-SNAPSHOT.jar /input
hadoop fs -put config.xml /input
hadoop fs -put data.txt /input
</code></pre>
                <ul>
                    <li>运行程序</li>
                </ul>
                <pre><code>hadoop jar /opt/hadoop/k-means-1.0-SNAPSHOT.jar it.unipi.hadoop.KMeans /input/data.txt /output
</code></pre>
                <p><strong>注意：HDFS上不能有/output路径存在，如果有的话，使用下面的命令删去</strong></p>
                <pre><code>hadoop fs -rm -r /output
</code></pre>
<img src="static/assets/img/hadoop_spark/hadoop_kmeans1.png" alt="hadoop_kmeans1" style="width: 100%; height: auto;"/>
<br><br>
<img src="static/assets/img/hadoop_spark/hadoop_kmeans2.png" alt="hadoop_kmeans2" style="width: 100%; height: auto;"/>
<br><br>
<img src="static/assets/img/hadoop_spark/hadoop_kmeans3.png" alt="hadoop_kmeans3" style="width: 100%; height: auto;"/>
                <ul>
                    <li>从HDFS上下载并查看结果文件</li>
                </ul>
                <pre><code>hadoop fs -get /user/Hadoop/output/centroids.txt
cat centroids.txt
</code></pre>
<img src="static/assets/img/hadoop_spark/hadoop_kmeans4.png" alt="hadoop_kmeans4" style="width: 100%; height: auto;"/>
<br><br>
                <p>注意，每次k-means算法的结果不一定一样，迭代次数也未必相同，能正常运行即可</p>
                <div style="height: 32px;"></div>
                <h1 id="spark"><strong>二、Spark</strong></h1>
                <div style="height: 32px;"></div>
                <h2 id="1-3"><strong>1、简介</strong></h2>
                <p>Apache Spark 是一个开源的分布式计算框架，主要用于大规模数据处理。Spark 提供了高效的计算引擎，支持批处理和流处理，同时还包括机器学习库、图计算库和SQL查询引擎等功能。</p>
                <img src="static/assets/img/hadoop_spark/spark_achi.png" alt="spark_achi" style="width: 100%; height: auto;"/>
                <h2 id="2spark"><strong>2、Spark的核心概念</strong></h2>
                <p><strong>（1）RDD（弹性分布式数据集）</strong></p>
                <p>RDD 是 Spark 中的核心抽象，它代表一个不可变的分布式数据集，可以并行处理。RDD 的数据可以存储在集群的各个节点上，并支持多种操作，如映射、过滤、聚合等。</p>
                <p><strong>（2）DataFrame 和 DataSet</strong></p>
                <ul>
                    <li>DataFrame：是 Spark SQL 提供的一个数据结构，类似于关系型数据库中的表格。它是一个分布式的数据集合，支持各种 SQL 查询、数据处理和转换操作。</li>
                    <li>DataSet：是 DataFrame 和 RDD 的结合。它既具有 RDD 的类型安全（即编译时检查类型），又具有 DataFrame 的优化执行引擎</li>
                </ul>
                <p><strong>（3）转换（Transformation） 和 行动（Action）</strong></p>
                <ul>
                    <li>转换：是 Spark 中对 RDD、DataFrame 或 DataSet 进行的操作，它们是惰性执行的，即转换操作不会立即计算结果，而是创建一个新的 RDD、DataFrame 或
                        DataSet，记录下对原数据集的操作。这些操作会构建起一个执行计划，直到调用行动操作时才会真正执行。</li>
                    <li>行动：是 Spark 中会触发实际计算的操作。它们会返回最终的结果，并且会启动对数据的实际计算过程。行动操作执行时，会计算整个数据集，通常会产生一个输出结果，或者将数据保存到外部存储中。
                    </li>
                </ul>
                <p><strong>（4）Spark Streaming</strong></p>
                <ul>
                    <li>Spark Streaming 是 Spark 用于处理实时数据流的组件。它将实时数据流分割成小批量的数据块（micro-batch），并利用 Spark 的批处理引擎来处理这些小批量数据。
                    </li>
                </ul>
                <p><strong>（5）机器学习库（MLlib）</strong></p>
                <ul>
                    <li>MLlib 是 Spark 提供的机器学习库，包含了常见的机器学习算法，如分类、回归、聚类、推荐等。</li>
                </ul>
                <p><strong>（6）GraphX</strong></p>
                <ul>
                    <li>GraphX 是 Spark 提供的图计算库，支持大规模的图数据处理。GraphX 提供了图的表示和图操作的 API，支持常见的图算法，如 PageRank、连接组件、最短路径等。</li>
                </ul>
                <h2 id="3minikubespark"><strong>3、使用minikube部署Spark</strong></h2>
                <p><strong>（1）向minikube中导入所需的镜像（这一步在已在上面的Hadoop部署过程中做过了，导入的镜像是一样的，可以跳过）</strong></p>
                <pre><code>minikube image load spark-hadoop-numpy:latest
</code></pre>
                <ul>
                    <li>进入minikube查看是否导入</li>
                </ul>
                <pre><code>minikube ssh
docker images
</code></pre>
<img src="static/assets/img/hadoop_spark/spark_image.png" alt="spark_image" style="width: 100%; height: auto;"/>
<br><br>
                <p><strong>（2）进入之前所建立的Hadoop集群中的namenode的命令行bash，更改hdfs的权限，让spark有权限访问hdfs，并创建spark-logs目录以存放spark的历史任务日志。下面命令中需要替换为自己的namenode所在pod的名称</strong>
                </p>
                <pre><code>kubectl exec -it namenode-78587cfc8f-v4zk6
hdfs dfs -mkdir -p /spark-logs
hdfs dfs -chmod 1777 /
</code></pre>
<img src="static/assets/img/hadoop_spark/spark_hdfs1.png" alt="spark_hdfs1" style="width: 100%; height: auto;"/>
<img src="static/assets/img/hadoop_spark/spark_hdfs2.png" alt="spark_hdfs2" style="width: 100%; height: auto;"/>
<br><br>
                <p><strong>（3）使用spark-yarn.yaml文件部署Spark，并查看pod状态验证其是否正常运行</strong></p>
                <pre><code>kubectl apply -f spark-yarn.yaml
kubectl get pods
</code></pre>
<img src="static/assets/img/hadoop_spark/spark_run.png" alt="spark_run" style="width: 100%; height: auto;"/>
<br><br>
                <p><strong>（4）运行SparkPI示例程序</strong></p>
                <ul>
                    <li>进入nodemanager的命令行bash</li>
                </ul>
                <pre><code>kubectl exec -it nodemanager-0 -- /bin/bash
</code></pre>
                <ul>
                    <li>提交spark PI示例作业</li>
                </ul>
                <pre><code>spark-submit --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi /opt/spark/examples/jars/spark-examples\_2.12-3.5.1.jar 10
</code></pre>
<img src="static/assets/img/hadoop_spark/spark_pi.png" alt="spark_pi" style="width: 100%; height: auto;"/>
<br><br>
                <p><strong>（5）运行k-means算法的Spark版本</strong></p>
                <ul>
                    <li>先退出nodemanager的bash</li>
                </ul>
                <pre><code>exit
</code></pre>
                <ul>
                    <li>再复制所需的文件到nodemanager中，下面的/home/zhxu/spark需要替换为自己电脑上的路径</li>
                </ul>
                <pre><code>kubectl cp /home/zhxu/spark/ nodemanager-0:/opt/spark/work-dir/
</code></pre>
                <ul>
                    <li>再次进入nodemanager的bash</li>
                </ul>
                <pre><code>kubectl exec -it nodemanager-0 -- /bin/bash
</code></pre>
                <ul>
                    <li>进入复制的文件夹下</li>
                </ul>
                <pre><code>cd spark
</code></pre>
                <ul>
                    <li>上传所用的文件到hdfs上</li>
                </ul>
                <pre><code>hdfs dfs -put data.txt /input/
</code></pre>
                <ul>
                    <li>提交作业</li>
                </ul>
                <pre><code>spark-submit --master yarn --deploy-mode client --files config.json --py-files point.py spark.py hdfs:///input/data.txt ./result.txt
</code></pre>
<img src="static/assets/img/hadoop_spark/spark_kmeans.png" alt="spark_kmeans" style="width: 100%; height: auto;"/>


            </div>

        </div>
    </section>
    <!-- Awards -->


    <!-- Footer-->
    <footer class="bg-bottom text-center py-5">
        <div class="container px-5">
            <div class="text-white-50 small">
                <div id="copyright-text" class="mb-2"></div>
                <a id="github-link" href="https://github.com/senli1073">Github</a>
                <span class="mx-1">&middot;</span>
                <a id="license-link"
                    href="https://github.com/senli1073/senli1073.github.io/blob/main/LICENSE">License</a>
            </div>
        </div>
    </footer>

</body>

</html>